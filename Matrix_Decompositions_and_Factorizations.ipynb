{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix decompositions \n",
    "Useful tool for reducing a matrix to their constituent parts in order to simplify more complex operations like computing the power of the matrix.<br>\n",
    "Original matrix in which we are interested may be very 'big', sparse, with no order. Factoring it would yield a set of more manageable, compact and ordered matrices. Further, using these factors, it is easy to find hidden relationships (if any) like correlation, orthogonality, sub-space/ span/ projection in the original matrix.\n",
    "\n",
    "## LU Matrix Decomposition\n",
    "For square matrices and decomposes a matrix into triangular matrices L and U.\n",
    "<br>\n",
    "The LU decomposition is found using an iterative numerical process and can fail for those matrices that cannot be decomposed or decomposed easily. A variation of this decomposition that is numerically more stable to solve in practice is called the **LUP decomposition**, or the LU decomposition with partial pivoting.\n",
    "<br>\n",
    "In the Gaussian elimination process, when there's a zero on the pivot, we have to switch rows, which introduces a Permutation matrix P. Also, very small non-zero pivot values lead to numerical instability in a floating point environment. Basic algorithms avoid this by searching for the entry with the largest absolute value in the pivot column and switching the corresponding row with the pivot row. This switch can be expensive, so often the largest absolute value entry will have to be bigger than the pivot's absolute value by some factor, e.g. 10, for the switch to occur. This reduces the number of switches, but keeps those that would be necessary to limit floating point errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Original Matrix===========\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "from scipy.linalg import lu\n",
    "# define matrix\n",
    "print '==========Original Matrix==========='\n",
    "A = array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========LUP Decomposition===========\n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]]\n",
      "[[ 1.          0.          0.        ]\n",
      " [ 0.14285714  1.          0.        ]\n",
      " [ 0.57142857  0.5         1.        ]]\n",
      "[[  7.00000000e+00   8.00000000e+00   9.00000000e+00]\n",
      " [  0.00000000e+00   8.57142857e-01   1.71428571e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   1.11022302e-16]]\n",
      "\n",
      "==========Reconstructed Matrix===========\n",
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7.  8.  9.]]\n"
     ]
    }
   ],
   "source": [
    "# LU decomposition\n",
    "P, L, U = lu(A)\n",
    "print '==========LUP Decomposition==========='\n",
    "print(P)\n",
    "print(L)\n",
    "print(U)\n",
    "# reconstruct\n",
    "B = P.dot(L).dot(U)\n",
    "print '\\n==========Reconstructed Matrix==========='\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDU Decomposition\n",
    "Just normalize the U matrix to pull out D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== D & New U ===========\n",
      "[[ 7.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[[ 1.          1.14285714  1.28571429]\n",
      " [ 0.          1.          2.        ]\n",
      " [ 0.          0.          1.        ]]\n",
      "\n",
      "==========Reconstructed Matrix===========\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.],\n",
       "       [ 4.,  5.,  6.],\n",
       "       [ 7.,  8.,  9.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = np.diag(np.diag(U))   # D is just the diagonal of U\n",
    "U /= np.diag(U)[:, None]  # Normalize rows of U\n",
    "print '========== D & New U ==========='\n",
    "print np.round(D)\n",
    "print U\n",
    "print '\\n==========Reconstructed Matrix==========='\n",
    "P.dot(L.dot(D.dot(U))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR Matrix Decomposition\n",
    "For m x n matrices (not limited to square matrices)<br>\n",
    "Q: m x m <br>\n",
    "R: m x n upper triangle matrix \n",
    "\n",
    "NumPy qr() function: By default, the function returns the Q and R matrices with smaller or ‘reduced’ dimensions that is more economical. We can change this to return the expected sizes of m x m for Q and m x n for R by specifying the mode argument as ‘complete’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Original Matrix===========\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "==========QR Decomposition===========\n",
      "[[-0.16903085  0.89708523  0.40824829]\n",
      " [-0.50709255  0.27602622 -0.81649658]\n",
      " [-0.84515425 -0.34503278  0.40824829]]\n",
      "[[-5.91607978 -7.43735744]\n",
      " [ 0.          0.82807867]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "==========Reconstructed Matrix===========\n",
      "[[ 1.  2.]\n",
      " [ 3.  4.]\n",
      " [ 5.  6.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import qr\n",
    "# define a 3x2 matrix\n",
    "AA = array([[1, 2], [3, 4], [5, 6]])\n",
    "print '==========Original Matrix==========='\n",
    "print(AA)\n",
    "# QR decomposition\n",
    "Q, R = qr(AA, 'complete')\n",
    "print '==========QR Decomposition==========='\n",
    "print(Q)\n",
    "print(R)\n",
    "# reconstruct\n",
    "B = Q.dot(R)\n",
    "print '\\n==========Reconstructed Matrix==========='\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cholesky Decomposition\n",
    "For hermitian (square symmetric if real valued) AND positive definite matrices (all values > 0)\n",
    "\n",
    "Lower Triangular matrix form (Numpy Default):\n",
    "A = L . L^T  <br>\n",
    "Upper Triangular matrix form (Scipy Default):\n",
    "A = U^T . U\n",
    "\n",
    "#### Applications: <br>\n",
    "1) Used for solving normal equations for linear least squares (eg in linear regression), as well as simulation and optimization methods <br>\n",
    "2) Cholesky transformation - to correlate and uncorrelate variables <br>\n",
    "https://blogs.sas.com/content/iml/2012/02/08/use-the-cholesky-transformation-to-correlate-and-uncorrelate-variables.html <br>\n",
    "3) Monte Carlo simulation - The correlation matrix is decomposed, to give the lower-triangular L. Applying this to a vector of uncorrelated samples u produces a sample vector Lu with the covariance properties of the system being modeled <br>\n",
    "4) Kalman filters - Unscented Kalman filters commonly use the Cholesky decomposition to choose a set of so-called sigma points. The Kalman filter tracks the average state of a system as a vector x of length N and covariance as an N × N matrix P. The matrix P is always positive semi-definite and can be decomposed into LL.T. The columns of L can be added and subtracted from the mean x to form a set of 2N vectors called sigma points. These sigma points completely capture the mean and covariance of the system state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Original Matrix ===========\n",
      "[[2 1 1]\n",
      " [1 2 1]\n",
      " [1 1 2]]\n",
      "========== Numpy Cholesky Decomposition ===========\n",
      "[[ 1.41421356  0.          0.        ]\n",
      " [ 0.70710678  1.22474487  0.        ]\n",
      " [ 0.70710678  0.40824829  1.15470054]]\n",
      "\n",
      "==========Reconstructed Matrix===========\n",
      "[[ 2.  1.  1.]\n",
      " [ 1.  2.  1.]\n",
      " [ 1.  1.  2.]]\n",
      "\n",
      "========== Scipy Cholesky Decomposition ===========\n",
      "[[ 1.41421356  0.70710678  0.70710678]\n",
      " [ 0.          1.22474487  0.40824829]\n",
      " [ 0.          0.          1.15470054]]\n",
      "\n",
      "==========Reconstructed Matrix===========\n",
      "[[ 2.  1.  1.]\n",
      " [ 1.  2.  1.]\n",
      " [ 1.  1.  2.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import cholesky\n",
    "# define a 3x3 matrix\n",
    "AAA = array([[2, 1, 1], [1, 2, 1], [1, 1, 2]])\n",
    "print '========== Original Matrix ==========='\n",
    "print(AAA)\n",
    "# Numpy Cholesky decomposition: L is default\n",
    "L = cholesky(AAA)\n",
    "print '========== Numpy Cholesky Decomposition ==========='\n",
    "print(L)\n",
    "# reconstruct\n",
    "B = L.dot(L.T)\n",
    "print '\\n==========Reconstructed Matrix==========='\n",
    "print(B)\n",
    "\n",
    "# Scipy Cholesky Decomposition: U is default \n",
    "from scipy.linalg import cholesky\n",
    "U = cholesky(AAA)\n",
    "print '\\n========== Scipy Cholesky Decomposition ==========='\n",
    "print(U)\n",
    "# reconstruct\n",
    "B = np.dot(U.T,U)\n",
    "print '\\n==========Reconstructed Matrix==========='\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative matrix factorization (NMF) \n",
    "decomposes a matrix A into two matrices that have non-negative elements. (A must have non-negative elements too). NMF is suited for tasks where the underlying factors can be interpreted as non-negative.  <br>\n",
    "The factorization is not unique.  <br>\n",
    "\n",
    "https://www.siam.org/meetings/sdm11/park.pdf\n",
    "\n",
    "#### NMF Applications\n",
    "1) Dimensionality Reduction <br>\n",
    "2) Text mining - Topic modeling, Documnet Clustering <br>\n",
    "3) Image analysis and computer vision - Feature representation, sparse coding <br>\n",
    "4) Latent feature extraction <br>\n",
    "5) Social network - Community structure and trend detection, Recommendation system <br>\n",
    "6) Acoustic signal processing, blind source separating <br>\n",
    "7) Financial data https://www.researchgate.net/publication/228656913_Analysis_of_financial_data_using_non-negative_matrix_factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Original Matrix ===========\n",
      "[[5 3 0 1]\n",
      " [4 0 0 1]\n",
      " [1 1 0 5]\n",
      " [1 0 0 4]\n",
      " [0 1 5 4]]\n",
      "\n",
      "========== NMF Decomposition ===========\n",
      "[[  0.00000000e+00   7.36983291e-01   1.68362443e+00   1.60383211e-01]\n",
      " [  0.00000000e+00   1.51492480e+00   0.00000000e+00   6.41010416e-03]\n",
      " [  9.04799081e-04   0.00000000e+00   5.53924595e-01   1.53101825e+00]\n",
      " [  0.00000000e+00   3.78732207e-01   0.00000000e+00   1.15003851e+00]\n",
      " [  4.68844828e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n",
      "[[ 0.          0.21329038  1.06645088  0.85316074]\n",
      " [ 2.64058825  0.          0.          0.64628388]\n",
      " [ 1.81306226  1.78222468  0.          0.        ]\n",
      " [ 0.          0.00519399  0.          3.26530094]]\n",
      "\n",
      "==========Reconstructed Matrix===========\n",
      "[[ 5.  3.  0.  1.]\n",
      " [ 4.  0.  0.  1.]\n",
      " [ 1.  1.  0.  5.]\n",
      " [ 1.  0.  0.  4.]\n",
      " [ 0.  1.  5.  4.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "A = [\n",
    "     [5,3,0,1],\n",
    "     [4,0,0,1],\n",
    "     [1,1,0,5],\n",
    "     [1,0,0,4],\n",
    "     [0,1,5,4],\n",
    "    ]\n",
    "A = array(A)\n",
    "print '========== Original Matrix ==========='\n",
    "print(A)\n",
    "nmf = NMF()\n",
    "W = nmf.fit_transform(A)\n",
    "print '\\n========== NMF Decomposition ==========='\n",
    "print W\n",
    "H = nmf.components_\n",
    "print H\n",
    "print '\\n==========Reconstructed Matrix==========='\n",
    "new_A = np.dot(W,H)\n",
    "print np.round(new_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling with NMF\n",
    "Displays the top words in a topic. Topics are not labeled by the algorithm — a numeric index is assigned.\n",
    "\n",
    "NMF has an inherent clustering property, such that W and H represent the following information about A:\n",
    "\n",
    "A (Document-word matrix) — input that contains which words appear in which documents.\n",
    "\n",
    "W (Basis vectors) — the topics (clusters) discovered from the documents.\n",
    "\n",
    "H (Coefficient matrix) — the membership weights for the topics in each document.\n",
    "\n",
    "Another interesting property of NMF is that it naturally produces sparse representations. This makes sense in the case of topic modeling: documents generally do not contain a large number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:people time right did good said say make way government\n",
      "Topic 2:window problem using server application screen display motif manager running\n",
      "Topic 3:god jesus bible christ faith believe christian christians sin church\n",
      "Topic 4:game team year games season players play hockey win league\n",
      "Topic 5:new 00 sale 10 price offer shipping condition 20 15\n",
      "Topic 6:thanks mail advance hi looking info help information address appreciated\n",
      "Topic 7:windows file files dos program version ftp ms directory running\n",
      "Topic 8:edu soon cs university ftp internet article email pub david\n",
      "Topic 9:key chip clipper encryption keys escrow government public algorithm nsa\n",
      "Topic 10:drive scsi drives hard disk ide floppy controller cd mac\n",
      "Topic 11:just ll thought tell oh little fine work wanted mean\n",
      "Topic 12:does know anybody mean work say doesn help exist program\n",
      "Topic 13:card video monitor cards drivers bus vga driver color memory\n",
      "Topic 14:like sounds looks look bike sound lot things really thing\n",
      "Topic 15:don know want let need doesn little sure sorry things\n",
      "Topic 16:car cars engine speed good bike driver road insurance fast\n",
      "Topic 17:ve got seen heard tried good recently times try couple\n",
      "Topic 18:use used using work available want software need image data\n",
      "Topic 19:think don lot try makes really pretty wasn bit david\n",
      "Topic 20:com list dave internet article sun hp email ibm phone\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer#, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 20\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "no_top_words = 10\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    topic_str = \" \".join([tfidf_feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "    print \"Topic %d:%s\" % (topic_idx+1, topic_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-convexity\n",
    "NMF algo is known be non-convex and NP-hard. So, it may have difficulty with convergence and stability. <br>\n",
    "One approach to tackle this is using SVD initialization (NNDSVD) <br>\n",
    "\n",
    "https://calculatedcontent.com/2013/05/06/advances-in-convex-nmf-part-1-linear-programming/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral (Eigen) Decomposition\n",
    "Eigendecomposition is a type of decomposition that involves decomposing a **square** matrix into a set of eigenvectors and eigenvalues. This can help us to analyze certain properties of the matrix, much as decomposing an integer into its prime factors can help us understand the behavior of that integer.\n",
    "\n",
    "A matrix could have one eigenvector and eigenvalue for each dimension of the parent matrix. Not all square matrices can be decomposed. The parent matrix can be shown to be a product of the eigenvectors and eigenvalues.\n",
    "\n",
    "A vector is an eigenvector of a matrix if it satisfies the following equation\n",
    "A . v = lambda . v\n",
    "\n",
    "A = Q . diag(V) . Q^-1\n",
    "\n",
    "Where Q is a matrix comprised of the eigenvectors, diag(V) is a diagonal matrix comprised of the eigenvalues along the diagonal (sometimes represented with a capital lambda), and Q^-1 is the inverse of the matrix comprised of the eigenvectors.\n",
    "\n",
    "Eigenvalues are coefficients applied to eigenvectors that give the vectors their length or magnitude. For example, a negative eigenvalue may reverse the direction of the eigenvector as part of scaling it. A matrix that has only positive eigenvalues is referred to as a _positive definite matrix_, whereas if the eigenvalues are all negative, it is referred to as a _negative definite matrix_.\n",
    "\n",
    "Eigenvectors returned by software packages are unit vectors (i.e. their length or magnitude is equal to 1.0). They are often referred as right vectors, which simply means a column vector (as opposed to a row vector or a left vector).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Original Matrix ===========\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "========== Eigen Values ===========\n",
      "[  1.61168440e+01  -1.11684397e+00  -1.30367773e-15]\n",
      "\n",
      "========== Orthonormal Eigen Vectors ===========\n",
      "[[-0.23197069 -0.78583024  0.40824829]\n",
      " [-0.52532209 -0.08675134 -0.81649658]\n",
      " [-0.8186735   0.61232756  0.40824829]]\n",
      "\n",
      "==========Reconstructed Matrix===========\n",
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7.  8.  9.]]\n"
     ]
    }
   ],
   "source": [
    "# eigendecomposition\n",
    "from numpy.linalg import eig\n",
    "print '========== Original Matrix ==========='\n",
    "A = array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print A\n",
    "# calculate eigendecomposition\n",
    "values, vectors = eig(A)\n",
    "print '\\n========== Eigen Values ==========='\n",
    "print(values)\n",
    "print '\\n========== Orthonormal Eigen Vectors ==========='\n",
    "print(vectors)\n",
    "# Reconstruct Original Matrix given only the eigenvectors and eigenvalues\n",
    "print '\\n==========Reconstructed Matrix==========='\n",
    "from numpy import diag, dot\n",
    "from numpy.linalg import inv \n",
    "\n",
    "# create diagonal matrix from eigenvalues. (The eigenvalues need to be arranged into a diagonal matrix)\n",
    "L = diag(values)\n",
    "# reconstruct the original matrix\n",
    "B = vectors.dot(L).dot(inv(vectors))\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Covariance matrix** is a square and symmetric matrix that summarises the variance between two variables. \n",
    "Eigenvectors with the largest eigenvalue of a covariance matrix gives the direction along which the data has the largest variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigen decomposition vs singular value decomposition\n",
    "In an eigen-decomposition, A can be represented by a product of its eigenvectors Q and diagonalised eigenvalues Λ:<br> <br>\n",
    "A=QΛ(Q^−1) \n",
    "<br> <br>\n",
    "--Unlike an eigen-decomposition where the matrix must be square (n×n), SVD can decompose a matrix of any dimension.<br>\n",
    "--Column vectors in Q are not always orthogonal so the change in basis is not a simple rotation. U and V are orthogonal and always represent rotations (and reflections).<br>\n",
    "--Singular values in S are all real and non-negative. The eigenvalues in Λ can be complex and have imaginary numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Applications\n",
    "1) Using SVD for image compression.<br>\n",
    "Checkout: https://github.com/abhi25t/machine_learning/blob/master/SVD.ipynb\n",
    "\n",
    "2) **Dimensionality Reduction/PCA** The principal components correspond the the largest eigenvalues of A'A and this yields the least squared projection onto a smaller dimensional hyperplane, and the eigenvectors become the axes of the hyperplane. Dimensionality reduction is extremely useful in machine learning and data analysis as it allows one to understand where most of the variation in the data comes from.\n",
    "\n",
    "3) **Low rank factorization for collaborative prediction** This what Netflix does (or once did) to predict what rating you'll have for a movie you have not yet watched. It uses the SVD, and throws away the smallest eigenvalues of A'A.\n",
    "\n",
    "4) **The Google Page Rank algorithm** The largest eigenvector of the graph of the internet is how the pages are ranked.\n",
    "\n",
    "5) **Spectral clustering** - It makes use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. <br>\n",
    "The goal of spectral clustering is to cluster data that is connected but not necessarily compact or clustered within convex boundaries. <br> The power of Spectral Clustering is to identify non-compact clusters in  a single data set. <br>\n",
    "Checkout: https://github.com/abhi25t/machine_learning/blob/master/Spectral_Clustering.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Read\n",
    "\n",
    "Section 2.7 Eigendecomposition [Deep Learning (Ian Goodfellow, Yoshua Bengio, Aaron Courville)]\n",
    "\n",
    "3Blue1Brown - A visual understanding of eigenvectors, eigenvalues\n",
    "https://www.youtube.com/watch?v=PFDu9oVAE-g\n",
    "\n",
    "To check Complex topics: <br>\n",
    "https://calculatedcontent.com/2012/09/09/dimensional-reduction-via-an-effective-operator-kernel/ <br>\n",
    "https://calculatedcontent.com/2012/09/20/effective-operators-for-eigenvalues-and-clustering/ <br>\n",
    "https://calculatedcontent.com/2012/09/21/eigenvalue-independent-effective-semantic-operator/ <br>\n",
    "\n",
    "Matrix multiplication meaning:\n",
    "http://alyssaq.github.io/2015/visualising-matrices-and-affine-transformations-with-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
