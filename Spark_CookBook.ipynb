{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### History:\n",
    "Developed by Matei Zaharia as a part of his PhD thesis at UC Berkeley starting in 2009. <br>\n",
    "First version released in 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkContext as an instance method\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API: Data Structures \n",
    "\n",
    "#### RDD: \n",
    "The core data structure in Spark is a distributed collection of immutable JVM (Java Virtual Machine) objects called Resilient Distributed Datasets (RDD). The Python data is stored within these JVM objects through py4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = [('A', 85), ('B', 47), ('C',15), ('D', 2), ('A', 19)]\n",
    "# Below will generate a ParallelCollectionRDD\n",
    "distributed_data = sc.parallelize(raw_data)\n",
    "\n",
    "# Below will genearte a MapPartitionsRDD\n",
    "\n",
    "# text_file_address = 'textfile.txt' \n",
    "# dist_file = sc.textFile(text_file_address, 4) \n",
    "\n",
    "# The last parameter denotes the number of partitions to cut the dataset into.\n",
    "# Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. \n",
    "# Normally, Spark tries to set the number of partitions automatically based on your cluster. \n",
    "# However, you can also set it manually by passing it as a second parameter to parallelize\n",
    "\n",
    "# Each row from the file forms an element of an RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Transformations\n",
    "\n",
    "#### .map()\n",
    "The method is applied to each element of the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85, 44, 15]\n",
      "[87, 46, 17, 4, 21]\n"
     ]
    }
   ],
   "source": [
    "list_data = [85, 44, 15, 2, 19]\n",
    "distributed_list = sc.parallelize(list_data)\n",
    "mapped_data = distributed_list.map(lambda x: x+2)\n",
    "\n",
    "# We then use the take() method to print the first n elements of the RDD\n",
    "print distributed_list.take(3)\n",
    "\n",
    "#The .collect() method fetches the entire RDD to a single machine (the driver) where it is converted to a list.\n",
    "collected_data = mapped_data.collect()\n",
    "print collected_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .reduceByKey(func)\n",
    "Merge the values for each key using an associative and commutative reduce function. <br>\n",
    "When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AA', 2), ('BA', 1), ('CA', 1), ('DA', 1)]\n"
     ]
    }
   ],
   "source": [
    "dict_data = [('AA', 85), ('BA', 47), ('CA',15), ('DA', 2), ('AA', 19)]\n",
    "distributed_dict = sc.parallelize(dict_data)\n",
    "histogram_data = distributed_dict.map(lambda x: (x[0], 1)).reduceByKey(lambda x,y: x+y)\n",
    "collected_data = histogram_data.collect()\n",
    "print collected_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .filter(func)\n",
    "Return a new dataset formed by selecting those elements of the source on which func returns true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44, 2]\n",
      "[('BA', 1)]\n"
     ]
    }
   ],
   "source": [
    "even_list = distributed_list.filter(lambda x: x % 2 == 0).collect()\n",
    "print even_list\n",
    "\n",
    "filtered_histogram_data = distributed_dict.filter(lambda x: 'B' in x[0]).map(lambda x: (x[0], 1)).reduceByKey(lambda x,y: x+y)\n",
    "print filtered_histogram_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .flatMap(f, preservesPartitioning=False)\n",
    "Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 1, 2, 3]\n",
      "[(2, 3), (12, 22), (3, 4), (13, 23), (4, 5), (14, 24)]\n",
      "{2: 3, 3: 4, 4: 5, 12: 22, 13: 23, 14: 24}\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([2, 3, 4])\n",
    "print rdd.flatMap(lambda x: range(1, x)).collect()\n",
    "print rdd.flatMap(lambda x: [(x, x+1), (x+10, x+20)]).collect()\n",
    "\n",
    "# Return the key-value pairs in this RDD to the master as a dictionary\n",
    "print rdd.flatMap(lambda x: [(x, x+1), (x+10, x+20)]).collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .distinct(numPartitions=None)\n",
    "Return a new RDD containing the distinct elements in this RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .sample(withReplacement, fraction, seed=None)\n",
    "Return a sampled subset of this RDD.\n",
    "\n",
    "Parameters:\t\n",
    "-  withReplacement – can elements be sampled multiple times (replaced when sampled out)\n",
    "-  fraction – expected size of the sample as a fraction of this RDD’s size without replacement: probability that each element is chosen; fraction must be [0, 1] with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
    "-  seed – seed for the random number generator\n",
    "\n",
    "Note: This is not guaranteed to provide exactly the fraction specified of the total count of the given DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 27, 40, 42, 43, 60, 76, 80, 86, 97]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(100), 4)\n",
    "print rdd.sample(False, 0.1, 81).collect()\n",
    "print rdd.sample(False, 0.1, 81).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .join(other, numPartitions=None)\n",
    "Return an RDD containing all pairs of elements with matching keys in self and other. <br>\n",
    "Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in self and (k, v2) is in other. <br>\n",
    "Performs a hash join across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', (1, 2)), ('a', (1, 3))]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "print sorted(x.join(y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### leftOuterJoin(other, numPartitions=None)\n",
    "Perform a left outer join of self and other. <br>\n",
    "For each element (k, v) in self, the resulting RDD will either contain all pairs (k, (v, w)) for w in other, or the pair (k, (v, None)) if no elements in other have key k. <br>\n",
    "Hash-partitions the resulting RDD into the given number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', (1, 2)), ('b', (4, None))]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "print sorted(x.leftOuterJoin(y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .intersection(other)\n",
    "Return the intersection of this RDD and another one. <br>\n",
    "The output will not contain any duplicate elements, even if the input RDDs did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
    "rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
    "rdd1.intersection(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### DataFrame: \n",
    "Like an RDD, a DataFrame is an immutable distributed collection of data. Data is organized into named columns, like a table in a relational database.\n",
    "#### Details: \n",
    "https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark SQL Catalyst Optimizer\n",
    "Powers both SQL queries and the DataFrame API. Two purposes: First, to make it easy to add new optimization techniques and features to Spark SQL. Second, to enable external developers to extend the optimizer — for example, by adding data source specific rules that can push filtering or aggregation into external storage systems, or support for new data types. \n",
    "Catalyst supports both rule-based and cost-based optimization.\n",
    "https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
